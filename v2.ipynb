{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Define custom dataset class\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "\n",
    "        self.alphabets = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz-' \"\n",
    "        self.max_str_len = 24  # max length of input labels\n",
    "        self.num_of_characters = len(\n",
    "            self.alphabets) + 1  # +1 for ctc pseudo blank\n",
    "        self.num_of_timestamps = 64  # max length of predicted labels\n",
    "\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.labels.dropna(axis=0, inplace=True)\n",
    "        self.labels = self.labels[self.labels['IDENTITY'] != 'UNREADABLE']\n",
    "        # self.labels['IDENTITY'] = self.labels['IDENTITY'].str.upper()\n",
    "        # self.labels = self.labels.iloc[:(len(self.labels)//batch_size)*batch_size]\n",
    "        self.labels = self.labels.iloc[:64]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    def label_to_num(self, label):\n",
    "        label_num = []\n",
    "        for ch in label:\n",
    "            label_num.append(self.alphabets.find(ch))\n",
    "\n",
    "        return np.array(label_num)\n",
    "\n",
    "    def num_to_label(self, num):\n",
    "        ret = \"\"\n",
    "        for ch in num:\n",
    "            if ch == -1:  # CTC Blank\n",
    "                break\n",
    "            else:\n",
    "                ret += self.alphabets[ch]\n",
    "        return ret\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        \n",
    "        (h, w) = img.shape\n",
    "\n",
    "        final_img = np.ones([256, 256])*255  # blank white image\n",
    "\n",
    "        # crop\n",
    "        if w > 256:\n",
    "            img = img[:, :256]\n",
    "\n",
    "        if h > 256:\n",
    "            img = img[:256, :]\n",
    "\n",
    "        final_img[:h, :w] = img\n",
    "        return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        image = ImageOps.grayscale(image)\n",
    "        # image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = image[-1, :, :]\n",
    "        # label = self.labels.iloc[idx, 1]\n",
    "        label = np.ones([self.max_str_len]) * -1\n",
    "        label[0:len(self.labels.iloc[idx, 1])] = self.label_to_num(\n",
    "            self.labels.iloc[idx, 1])\n",
    "        return self.preprocess(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator network\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, text_dim, image_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.image_size = image_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim + text_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, image_size * image_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text):\n",
    "        x = torch.cat([noise, text], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 1, self.image_size, self.image_size)\n",
    "        return x\n",
    "\n",
    "# Define discriminator network\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, text_dim, image_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.text_dim = text_dim\n",
    "        self.image_size = image_size\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * (image_size // 16) *\n",
    "                      (image_size // 16) + text_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # image = image.double()\n",
    "        # if (image.shape == torch.Size([64, 256, 256])):\n",
    "        #     image = image.unsqueeze(1)\n",
    "        if (len(image.shape) == 3):\n",
    "            image = image.unsqueeze(1)\n",
    "        # image = image.unsqueeze(1)\n",
    "        x = self.conv(image)\n",
    "        x = x.view(-1, 512 * (self.image_size // 16) * (self.image_size // 16))\n",
    "        x = torch.cat([x, text], dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0: D_loss=1.4983, G_loss=54.5758\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0002\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "noise_dim = 128\n",
    "text_dim = 24\n",
    "image_size = 256\n",
    "dataset_path = 'handwriting-recognition'\n",
    "\n",
    "# # Decide which device we want to run on\n",
    "ngpu = 0\n",
    "device = torch.device(\"cuda:0\" if (\n",
    "    torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# %%\n",
    "\n",
    "generator = Generator(noise_dim, text_dim, image_size).to(device)\n",
    "discriminator = Discriminator(text_dim, image_size).to(device)\n",
    "generator.double()\n",
    "discriminator.double()\n",
    "# generator.load_state_dict(torch.load('generator.pth', map_location=device))\n",
    "# discriminator.load_state_dict(torch.load('discriminator.pth', map_location=device))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion.double()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# %%\n",
    "train_dataset = HandwritingDataset(os.path.join(dataset_path, 'written_name_train_v2.csv'), os.path.join(dataset_path, 'train_v2', 'train'), transform=None)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = HandwritingDataset(os.path.join(dataset_path, 'written_name_validation_v2.csv'), os.path.join(dataset_path, 'validation_v2', 'validation'), transform=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# %%\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, text_labels) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        text_labels = text_labels.to(device)\n",
    "        # print(real_images, text_labels)\n",
    "        # break\n",
    "        # Train discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        real_labels = torch.ones(batch_size).to(device)\n",
    "        real_output = discriminator(real_images, text_labels)\n",
    "        real_labels = real_labels.double()\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "\n",
    "        # Fake images\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake_labels = torch.zeros(batch_size).to(device)\n",
    "        fake_images = generator(noise, text_labels)\n",
    "        fake_output = discriminator(fake_images.detach(), text_labels)\n",
    "        fake_labels = fake_labels.double()\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train generator\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake_labels = torch.ones(batch_size).to(device)\n",
    "        fake_images = generator(noise, text_labels)\n",
    "        fake_output = discriminator(fake_images, text_labels)\n",
    "        fake_labels = fake_labels.double()\n",
    "        g_loss = criterion(fake_output, fake_labels)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Print losses every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}: D_loss={d_loss.item():.4f}, G_loss={g_loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    val_losses = []\n",
    "    for i, (real_images, text_labels) in enumerate(val_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        text_labels = text_labels.to(device)\n",
    "\n",
    "        # Compute loss for discriminator\n",
    "        real_labels = torch.ones(batch_size).to(device)\n",
    "        real_output = discriminator(real_images, text_labels)\n",
    "        real_labels = real_labels.double()\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake_labels = torch.zeros(batch_size).to(device)\n",
    "        fake_images = generator(noise, text_labels)\n",
    "        fake_output = discriminator(fake_images, text_labels)\n",
    "        fake_labels = fake_labels.double()\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        val_losses.append(d_loss.item())\n",
    "\n",
    "    print(f\"Validation loss: {sum(val_losses)/len(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models to a file\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
    "\n",
    "test_image = generator(torch.randn(1, noise_dim).to(device), \"test\")\n",
    "plt.imshow(test_image[0].cpu().detach().numpy(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
